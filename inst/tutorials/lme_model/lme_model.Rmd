---
title: "Linear Mixed Effects in R"
author: "Christian Duenas"
date: "5/13/2021"
output: learnr::tutorial
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(learnr)
library(ucrstats) # When working on the tutorial, use your code below and comment this entire line.

# For deployment purposes, have the package load the data instead. 

# # #Load in the psthway to the datasets
# 
# root_dir <- dirname(dirname(dirname(getwd())))
# 
# # # Load in the data 
# load( paste0(root_dir, "/data/lme_one.RData") )
# lme_one$id <- as.factor(lme_one$id)
```

# Linear Mixed Effects Models 

```{r, message = FALSE, warning = FALSE}
library(lme4)
#head(lme_one)
```

## Introduction 

### Why Linear Mixed Effects

Up to this point, you've likely been using linear models to model your data. Perhaps it looks something like 

$Y = \beta X + \epsilon$

But when you create this model, there's a very important assumption you're making: linear models require that the data be **independent**. 

Sometimes it's safe to assume independence. In those cases, you're good to go. But what if it's not? 

Consider a longitudinal study where we are following children with some sort of rare disease as they grow older. In this scenario, each child will contribute multiple data points as time passes (we'll call these **replications**). Since replications are coming from the same person, they will likely be correlated. It's no longer safe to assume independence, meaning that our linear model is longer be appropriate to use. This is where *Linear Mixed Effect Models* (or *LME*) come to the rescue.

Let's illustrate another point. Suppose we have data from 3 individuals that look like this

![](images/ex.png)

What are some of the different ways we could interpret this data?

![If we look within each person, the points seem to have a positive trend.](images/ex1.png)

![If we look between each person, the trend seems to be negative](images/ex2.png)

If we fit a line to this data using normal methods, we'd lose this aspect of the data and would likely get a poor fit. The power of LMEs is that they act as a middleground between the two plots above and can offer insight into both trends.

## The Model

LMEs utilize **fixed** and **random** effects.

+ **Fixed Effects** - A parameter that does not differ from person to person (can be thought of as population effects).

+ **Random Effects** - A parameter that does vary from person to person. These are treated as random variables, and are assumed to follow some sort of distribution (i.e. the Normal Distribution)

With that in mind, our model for an ith individual can be expressed as $Y_{i} = X_{i}'\beta + Z_{i}b_i + \epsilon_{ij}$ where

$Y_{i}$ is the response vector 

$X_{i}$ is the design matrix of fixed effect covariates

$\beta$ is a vector of the fixed-effects regression coefficients

$Z_{i}$ is the design matrix of random effects covariates, usually a subset of $X_i$

$b_i$ is a vector of the random-effects regression coefficients. It is treated as a random variable.

$\epsilon_{i}$ - is a vector of errors

### The Covariance Matrix

"Light Intro"

Separation between random covar and error covar

## Estimating Parameters

### Estimating using Maximum Likelihood

Maximum Likelihood is technique to estimate parameters by using the fixed data points and finding a value for the estimate that has the highest chance of being observed. This is done by maximizing what is known as the "likelihood function."

$\sum_{i=1}(Y_i - X_i \beta)'\Sigma_i^{-1}(Y_i - X_i \beta)$

We can maximize \Beta in this expression (which is the same as minimizing the entire expression since the $\beta$s are negative) with what is known as the Generalized Least Squares estimator of $\beta$:

$\hat{\beta} = \{ \sum_{i=1} (X_i'\Sigma_{i}^{-1}X_i) \}^{-1} \sum_{i=1} (X_i'\Sigma_{i}^{-1}y_i)$

In order to solve this, we'd either have to know $\Sigma_i$ or have an estimate of it. So let's see how we can approach that.

### Restricted Maximum Likelihood Function

There is a slight problem when estimating our covariance term $\Sigma_i$ using the Maximum Likelhihod method: the result will be biased, especially in small samples. To remedy this, we're going to use what is called the Restricted Maximum Likelihood Estimate of $\Sigma_i$. This is obtained by maximizing the following modified log-likelihood function:

$-\frac{1}{2}\log(\Sigma_{i}) -\frac{1}{2}\sum_{i=1} (y_i - X_i\hat{\beta})' \Sigma_{i}^{-1} (y_i - X_i\hat{\beta}) -\frac{1}{2}\log(\sum_{i=1} X_i'\Sigma_{i}^{-1}X_i)$

We won't delve too deep into the math, but the general idea is that when we perform a REML estimate of $\Sigma_i$, we remove all of the $\beta$ terms so that we have a function with only one variable to maximize. Once we have our REML estimate $\hat\Sigma_i$, we can then go back and solve for $\beta$ using our normal; generalized least squares estimate.

### Best Linear Unbiased Estimator (BLUP)

For our fixed parameters, we use the term "estimate". But $b_i$ is technically a random variable, so we're going to "predict" its value rather than estimate it. Predicting a random variable is essentially pedicting it's conditional mean, so we can get a good prediction of $b_i$ using:

$E(b_i|Y_i) = GZ'\Sigma_{i}^{-1}(Y_i-Xi\hat{\beta})$, where $\Sigma_{i} = Cov(Y_i) = Z_iGZ_i' + R_i$

If we use our REML estimate for the covaraince parameter, we call this the predictor the "empirical BLUP"

$\hat{b_i} =  \hat{G}Z'\hat\Sigma_{i}^{-1}(Y_i-Xi\hat{\beta})$


## Fitting the Model Using R

Lucky for us, the `lme4` package makes it so that building a LME model in R is not too different from creating a linear model using `lm`.

Lets take a look at our simulated data which is stored in the dataframe named `lme_one`


```{r}
lme_one
```

We can fit an LME with just an intercept using the `lmer` function from the `lme4` package. Here's a simple intercept model

```{r intercept, exercise = TRUE}
# Use the lme_one function to create a model
intercept_model <-  lme4::lmer(data = lme_one, formula = Y ~ 1 + (1 | id))
# Use summary to get a quick overview of that model
summary(intercept_model)
```

This model has two terms: a fixed effect intercept and a random effects intercept.

The fixed part of the formula is the `1`. This tells the function to create an intercept. The random effects part of the formula is `(1 | id)`. This creates a random intercept term based on each subject in the data. You can see the idea demonstrated with this plot.

![](images/ex3.png)

### Adding Fixed and Random Effects

Now lets build on this model and add some variables from our data. Suppose we want to X1 to our model as a fixed effect. See if you can fill in the following code to build that model.

Hint: Think about how you would add a coefficent to the model using `lm()`


```{r firstLME, exercise = TRUE}
my_model <- lme4::lmer(data = lme_one, formula = Y ~ 1)
summary(my_model)
```

```{r firstLME-solution}
my_model <- lme4::lmer(data = lme_one, formula = Y ~ X1 + (1 | id))
summary(my_model)
```

Let's keep building on this model. Now suppose that you want to add the time as a coefficent (the `time_id` column). It makes sense to think that observations within a subject at one time point is likely to be correlated with another time point for that same subject. So let's make time a random effect. 

```{r randomeMLE, exercise = TRUE}
my_model <- lme4::lmer(data = lme_one, formula = Y ~ 1 + (1 | id) )
summary(my_model)
```

```{r randomeMLE-solution}
my_model <- lme4::lmer(data = lme_one, formula = Y ~ X1 + time_id + (1 + time_id | id)  )
summary(my_model)
```

Once complete, the model will now have a slope term for the time.

One important thing to note when adding random effects is that we usually need to add a fixed effect term for that variable as well. Recall that $Z_i$ is typically a subset of $X_i$.

Try making you're on models using the other variables in the dataset.

```{r tryit, exercise = TRUE}
my_model <- lme4::lmer(data = lme_one) # Insert some variables into the formula
#summary(my_model)
```



